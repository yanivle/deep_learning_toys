{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "47f4a746f3e0a6542ca1cff07656c74218c49d229a167650747f6b9f80d5cb2d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Inspiration: https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85\n",
    "\n",
    "# Imports and globals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Basic functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096]])"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def softmax_stable(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p / np.sum(p)\n",
    "\n",
    "softmax = softmax_stable\n",
    "\n",
    "softmax(np.array([range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_dim:int, hidden_dim:int, output_dim:int):\n",
    "        # x, h, and y are all column vectors\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.Wxh = np.random.uniform(0, 1, (hidden_dim, input_dim))\n",
    "        self.Whh = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
    "        self.Why = np.random.uniform(0, 1, (output_dim, hidden_dim))\n",
    "        self.bh = np.random.uniform(0, 1, self.h_shape)\n",
    "        self.by = np.random.uniform(0, 1, self.y_shape)\n",
    "        self.reset_history()\n",
    "\n",
    "    @property\n",
    "    def x_shape(self): return (self.input_dim, 1)\n",
    "    \n",
    "    @property\n",
    "    def h_shape(self): return (self.hidden_dim, 1)\n",
    "\n",
    "    @property\n",
    "    def y_shape(self): return (self.output_dim, 1)\n",
    "\n",
    "    def reset_history(self):\n",
    "        h = np.zeros(self.h_shape)\n",
    "        self.h_history = [h]\n",
    "        self.y_history = []\n",
    "\n",
    "    @property\n",
    "    def seq_length(self):\n",
    "        return len(self.y_history)\n",
    "\n",
    "    @property\n",
    "    def h(self):\n",
    "        return self.h_history[-1]\n",
    "\n",
    "    @h.setter\n",
    "    def h(self, value):\n",
    "        assert value.shape == self.h_shape, (value.shape, self.h_shape)\n",
    "        self.h_history.append(value)\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.y_history[-1]\n",
    "\n",
    "    @y.setter\n",
    "    def y(self, value):\n",
    "        assert value.shape == (self.output_dim, 1), (value.shape, self.y_shape)\n",
    "        self.y_history.append(value)\n",
    "\n",
    "    def forward_one_step(self, x):\n",
    "        assert x.shape == self.x_shape, (x.shape, self.x_shape)\n",
    "        z = np.dot(self.Wxh, x) + np.dot(self.Whh, self.h) + self.bh\n",
    "        self.h = np.tanh(z)\n",
    "        o = np.dot(self.Why, self.h) + self.by\n",
    "        self.y = softmax(o)\n",
    "        return self.y\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return [self.forward_one_step(x) for x in xs]\n",
    "\n",
    "    def predict_one_step(self, x):\n",
    "        return np.argmax(self.forward_one_step(x))\n",
    "\n",
    "    def predict(self, xs):\n",
    "        return [self.predict_one_step(x) for x in xs]\n",
    "\n",
    "    def loss(self, targets):\n",
    "        return sum(-np.log(self.y_history[t][targets[t], 0]) for t in range(self.seq_length))\n",
    "\n",
    "    def backward(self, xs, targets):\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dparams = (dWxh, dWhh, dWhy, dbh, dby)\n",
    "        dhnext = np.zeros_like(self.h)\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(self.y_history[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, self.h_history[t].T)\n",
    "            dby += dby\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhrec = (1 - self.h_history[t] * self.h_history[t]) * dh\n",
    "            dbh += dhrec\n",
    "            dWxh += np.dot(dhrec, xs[t].T)\n",
    "            dWhh += np.dot(dhrec, self.h_history[t - 1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhrec)\n",
    "        for dparam in dparams:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dparams\n",
    "\n",
    "    def update(self, dparams, learning_rate):\n",
    "        dWxh, dWhh, dWhy, dbh, dby = dparams\n",
    "        self.Wxh -= dWxh * learning_rate\n",
    "        self.Whh -= dWhh * learning_rate\n",
    "        self.Why -= dWhy * learning_rate\n",
    "        self.bh -= dbh * learning_rate\n",
    "        self.by -= dby * learning_rate\n",
    "\n",
    "    def train(self, xs, targets, iters, learning_rate, print_every = 1000):\n",
    "            for i in range(iters):\n",
    "                self.reset_history()\n",
    "                preds = self.predict(xs)\n",
    "                loss = rnn.loss(targets)\n",
    "                if i % print_every == 0:\n",
    "                    print('Predictions:', preds, 'vs targets:', targets)\n",
    "                    print('Loss:', loss)\n",
    "                dparams = rnn.backward(xs, targets)\n",
    "                self.update(dparams, learning_rate)"
   ]
  },
  {
   "source": [
    "Let's try to learn the constant 0 function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 8.24779750773102\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.016003682858074433\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.007953920998736316\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.0052842645401016694\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.003953525732722958\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.0031568647670950495\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.002626694151580089\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.0022485536290676967\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.0019653014653172617\n",
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loss: 0.0017452334842126885\n"
     ]
    }
   ],
   "source": [
    "in_dim = 3\n",
    "out_dim = 3\n",
    "hidden_dim = 5\n",
    "length = 10\n",
    "\n",
    "def constant_dataset(c, length, in_out_dim):\n",
    "    xs = np.random.uniform(0, 1, (length, in_out_dim, 1))\n",
    "    targets = [c for _ in range(length)]\n",
    "    return xs, targets\n",
    "\n",
    "x_train, y_train = constant_dataset(0, length, in_dim)\n",
    "rnn = RNN(in_dim, hidden_dim, out_dim)\n",
    "rnn.train(x_train, y_train, 10000, 0.01)"
   ]
  },
  {
   "source": [
    "Let's now try to learn the constant function 1:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 9.192803410068464\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.016014932724079348\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.007952570889906118\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.005281430417648357\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.003950582686721133\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.003154086307012429\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.0026241300319009607\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.0022461973557770954\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.001963132619084393\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Loss: 0.0017432299154777802\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = constant_dataset(1, length, in_dim)\n",
    "rnn = RNN(in_dim, hidden_dim, out_dim)\n",
    "rnn.train(x_train, y_train, 10000, 0.01)"
   ]
  },
  {
   "source": [
    "And the constant 2 function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 16.252170119444525\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.01743541728278837\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.008559397861840328\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.005650382980376492\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.004209869439123079\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.0033513671005677213\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.002781967741385888\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.0023769326596804635\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.002074201251148503\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] vs targets: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loss: 0.0018394355271413824\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = constant_dataset(2, length, in_dim)\n",
    "rnn = RNN(in_dim, hidden_dim, out_dim)\n",
    "rnn.train(x_train, y_train, 10000, 0.01)"
   ]
  },
  {
   "source": [
    "Now let's try a 0-1 sequence:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_dataset(seq, length, in_out_dim):\n",
    "    xs = np.random.uniform(0, 1, (length, in_out_dim, 1))\n",
    "    targets = [seq[i % len(seq)] for i in range(length)]\n",
    "    return xs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: 10.392026718231847\n",
      "Predictions: [0, 2, 1, 1, 0, 0, 2, 2, 1, 0] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: 23.71322321868329\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: 600.6784858483779\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: 1464.7921827473867\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: 2329.2495269284645\n",
      "<ipython-input-330-07199ad8e609>:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return sum(-np.log(self.y_history[t][targets[t], 0]) for t in range(self.seq_length))\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: inf\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: inf\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: inf\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: inf\n",
      "Predictions: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1] vs targets: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Loss: inf\n"
     ]
    }
   ],
   "source": [
    "in_dim = 3\n",
    "out_dim = 3\n",
    "hidden_dim = 10\n",
    "length = 10\n",
    "\n",
    "x_train, y_train = periodic_dataset([1, 0], length, in_dim)\n",
    "rnn = RNN(in_dim, hidden_dim, out_dim)\n",
    "rnn.train(x_train, y_train, 10000, 0.01)"
   ]
  },
  {
   "source": [
    "Let's try repeating the input:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_dataset(length, in_out_dim):\n",
    "    xs = np.random.uniform(0, 2, (length, in_out_dim, 1))\n",
    "    targets = [np.round(xs.) for i in range(length)]\n",
    "    return xs, targets\n"
   ]
  }
 ]
}